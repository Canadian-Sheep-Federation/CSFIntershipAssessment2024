{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOisUJq8zrM6bYsfmgrW+uJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamytechInc/CSFIntershipAssessment2024/blob/main/Carrot_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing video pipeline\n",
        "This is an implementation of how I would adapt a model to record (in a .json file) carrots. It records the confidence of the model, the moment it detected it and the number of different carrots (using a feature matching algorithm). It uses a webcam and YOLOv5s as placeholders of a grocery camera and a carrot-detecting model, respectively.\n",
        "\n"
      ],
      "metadata": {
        "id": "Lw6O9btq4Rb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install cv2\n",
        "!pip install datetime\n",
        "!pip install pypi-json"
      ],
      "metadata": {
        "id": "hh_zlFR-4QuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "dBWH3H3R28SZ",
        "outputId": "4b0b97b9-298b-48fb-c0c2-deecbb554a64"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1676914b516a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;31m# Appease the type checker; ordinarily this binding is inserted by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load YOLOv5 model from Ultralytics repository\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
        "\n",
        "# Initialize ORB detector for feature matching\n",
        "orb = cv2.ORB_create()\n",
        "\n",
        "# Function to process live video and count carrots\n",
        "def process_live_video(camera_index=0, output_json='carrot_detections.json'):\n",
        "    # Open the webcam\n",
        "    cap = cv2.VideoCapture(camera_index)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error opening video stream or file\")\n",
        "        return\n",
        "\n",
        "    # Initialize data storage\n",
        "    data = {\"detections\": []}\n",
        "    carrot_descriptors = []\n",
        "\n",
        "    # Loop through video frames\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Preprocess the frame for YOLO\n",
        "        results = model(frame)\n",
        "\n",
        "        # Post-process the results to extract bounding boxes, labels, and confidence scores\n",
        "        labels, confidences, boxes = results.xyxyn[0][:, -1], results.xyxyn[0][:, -2], results.xyxyn[0][:, :-2]\n",
        "\n",
        "        # Iterate through detections and filter for carrots (assuming class 0 is carrot, this will depend on your model's class mappings)\n",
        "        for label, confidence, box in zip(labels, confidences, boxes):\n",
        "            if int(label) == 0:  # Assuming class 0 is carrot\n",
        "                # Extract bounding box coordinates\n",
        "                x1, y1, x2, y2 = int(box[0] * frame.shape[1]), int(box[1] * frame.shape[0]), int(box[2] * frame.shape[1]), int(box[3] * frame.shape[0])\n",
        "\n",
        "                # Crop the detected carrot region\n",
        "                carrot_region = frame[y1:y2, x1:x2]\n",
        "\n",
        "                # Detect and compute ORB descriptors\n",
        "                keypoints, descriptors = orb.detectAndCompute(carrot_region, None)\n",
        "\n",
        "                # Feature matching to count unique carrots\n",
        "                matches = []\n",
        "                if descriptors is not None:\n",
        "                    for existing_descriptors in carrot_descriptors:\n",
        "                        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "                        matches = bf.match(descriptors, existing_descriptors)\n",
        "                        if len(matches) > 10:  # Threshold to consider the same carrot\n",
        "                            break\n",
        "                    if len(matches) <= 10:\n",
        "                        carrot_descriptors.append(descriptors)\n",
        "\n",
        "                # Draw bounding box and label on the frame\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                cv2.putText(frame, f\"Carrot: {confidence:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "                # Print detection info\n",
        "                print(f\"Carrot detected with a confidence of {confidence:.2f} at {timestamp}.\")\n",
        "\n",
        "                # Record detection in JSON data\n",
        "                timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                detection = {\n",
        "                    \"timestamp\": timestamp,\n",
        "                    \"confidence\": float(confidence),\n",
        "                    \"bounding_box\": [x1, y1, x2, y2]\n",
        "                }\n",
        "                data[\"detections\"].append(detection)\n",
        "\n",
        "        # Display the frame with detections\n",
        "        cv2.imshow('Live Video', frame)\n",
        "\n",
        "        # Break the loop on 'q' key press\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    # Record the count of unique carrots\n",
        "    data[\"unique_carrots_count\"] = len(carrot_descriptors)\n",
        "\n",
        "    # Save data to JSON file\n",
        "    with open(output_json, 'w') as json_file:\n",
        "        json.dump(data, json_file, indent=4)\n",
        "\n",
        "    # Release video capture and close all OpenCV windows\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Start processing live video from the default webcam (camera index 0)\n",
        "process_live_video(camera_index=0, output_json='carrot_detections.json')\n"
      ]
    }
  ]
}