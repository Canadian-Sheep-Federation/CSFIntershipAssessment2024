{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Processing Video Pipeline\n",
        "\n",
        "Assume you have a working ML model that can process individual images and identify carrots, how would you adapt that model such that you could feed it live video inside a grocery store and have it create a record of any carrots it sees."
      ],
      "metadata": {
        "id": "yW5PHX76ZlcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To adapt the existing ML model to process live video feeds and identify carrots in a grocery store, I would integrate the model into a video processing pipeline using relevant AI and Python libraries. First, I would use OpenCV to capture and preprocess the video frames in real-time. Each image would be fed into the ML model, which would process the frames and identify any carrots present. Detected carrots would be logged into a record, including metadata such as the timestamp and location within the frame.\n",
        "\n",
        "To ensure efficient processing, the pipeline would utilize a buffer to manage the flow of frames, ensuring real-time analysis without significant lag. Additionally, optimizing the model for speed and accuracy would be crucial, potentially leveraging GPU acceleration to handle the high volume of data in real-time. This setup would enable continuous monitoring and recording of carrot detections within the grocery store environment."
      ],
      "metadata": {
        "id": "RSLWmpidZxXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo\n",
        "\n",
        "Write a toy implementation of whatever machine learning concept you would like in order to demonstrate your skills. This doesn't need to be in the notebook if you want to use something other than python.\n",
        "\n",
        "The problems we work on are wholly related to classfication, so your toy implementation should show knowledge of the fundamentals of classification problems."
      ],
      "metadata": {
        "id": "kvpVp6N_Zxqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Flatten the images to vectors of size 784\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "# Normalize the pixel values to range [0, 1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Function to calculate Euclidean distance\n",
        "def euclidean_distance(a, b):\n",
        "    return np.sqrt(np.sum((a - b) ** 2, axis=1))\n",
        "\n",
        "# k-NN classifier\n",
        "def knn(x_train, y_train, x_test, k=3):\n",
        "    predictions = []\n",
        "    for test_point in x_test:\n",
        "        distances = euclidean_distance(x_train, test_point)\n",
        "        # Get the indices of the k nearest neighbors\n",
        "        nearest_neighbors_indices = np.argsort(distances)[:k]\n",
        "        # Get the labels of the k nearest neighbors\n",
        "        nearest_labels = y_train[nearest_neighbors_indices]\n",
        "        # Get the most common label (majority vote)\n",
        "        unique, counts = np.unique(nearest_labels, return_counts=True)\n",
        "        majority_vote = unique[np.argmax(counts)]\n",
        "        predictions.append(majority_vote)\n",
        "    return np.array(predictions)\n",
        "\n",
        "# Use a subset of the test set for evaluation (e.g., 1000 samples)\n",
        "subset_size = 1000\n",
        "x_test_subset = x_test[:subset_size]\n",
        "y_test_subset = y_test[:subset_size]\n",
        "\n",
        "# Predict using k-NN with k=3\n",
        "k = 3\n",
        "y_pred = knn(x_train, y_train, x_test_subset, k=k)\n",
        "\n",
        "# Evaluate the accuracy\n",
        "accuracy = accuracy_score(y_test_subset, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2VuexdHbLyR",
        "outputId": "587cf68e-e993-499f-a663-45394b7633fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 96.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code implements a k-Nearest Neighbors (k-NN) classifier for the MNIST dataset using NumPy. First, the MNIST dataset is loaded using TensorFlow, and the images are flattened from 28x28 pixel arrays to 784-dimensional vectors. The pixel values are then normalized to the range [0, 1]. A helper function, euclidean_distance, computes the Euclidean distance between two points. The knn function classifies each test point by calculating distances from the test point to all training points, identifying the k nearest neighbors, and determining the most frequent label among these neighbors (majority vote). Finally, the classifier is evaluated on a subset of the test set, and its accuracy is calculated using the accuracy_score function from Scikit-learn. The accuracy is then printed."
      ],
      "metadata": {
        "id": "AIi6bsvDfZzS"
      }
    }
  ]
}