{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Processing Video Pipeline Answer**\n",
        "\n",
        "\n",
        "As we have working ML model to classify an object into carrot or not, if we want to use that in a video one thing we could do is we can have our video divided into each frame so it acts as an image and then use our model to see if that image contains carrot or not and we could do that for each frame of that video to detect it. But one problem I think with this approach is that if we have really slow detection model then this would take forever for video to process if video is too long.\n",
        "\n"
      ],
      "metadata": {
        "id": "7TEXCNtdy72g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Toy Implementation**"
      ],
      "metadata": {
        "id": "xWyCwMKIzS7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.loss import CrossEntropyLoss\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "import numpy as np\n",
        "import timeit\n",
        "from collections import OrderedDict\n",
        "from pprint import pformat\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "torch.multiprocessing.set_sharing_strategy('file_system')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "CrossELoss = F.cross_entropy\n",
        "\n",
        "learning_rate = 1e-3\n",
        "l1_lambda_1 = 3e-5\n",
        "\n",
        "def compute_score(acc, min_thres, max_thres):\n",
        "    if acc <= min_thres:\n",
        "        base_score = 0.0\n",
        "    elif acc >= max_thres:\n",
        "        base_score = 100.0\n",
        "    else:\n",
        "        base_score = float(acc - min_thres) / (max_thres - min_thres) \\\n",
        "                     * 100\n",
        "    return base_score\n",
        "\n",
        "\n",
        "def run(algorithm, dataset_name, filename):\n",
        "    start = timeit.default_timer()\n",
        "    predicted_test_labels, gt_labels = algorithm(dataset_name)\n",
        "    if predicted_test_labels is None or gt_labels is None:\n",
        "      return (0, 0, 0)\n",
        "    stop = timeit.default_timer()\n",
        "    run_time = stop - start\n",
        "\n",
        "    np.savetxt(filename, np.asarray(predicted_test_labels))\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for label, prediction in zip(gt_labels, predicted_test_labels):\n",
        "      total += label.size(0)\n",
        "      correct += (prediction.cpu().numpy() == label.cpu().numpy()).sum().item()\n",
        "    accuracy = float(correct) / total\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
        "    return (correct, accuracy, run_time)\n",
        "\n",
        "\n",
        "\n",
        "# Logistic regression\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self,input,output):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.fc = nn.Linear(input, output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class One_Hot(nn.Module):\n",
        "    def __init__(self, depth):\n",
        "        super(One_Hot,self).__init__()\n",
        "        self.depth = depth\n",
        "        self.ones = torch.sparse.torch.eye(depth).to(device)\n",
        "    def forward(self, X_in):\n",
        "        X_in = X_in.long()\n",
        "        return self.ones.index_select(0,X_in.data)\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + \"({})\".format(self.depth)\n",
        "\n",
        "one_hot = One_Hot(10).to(device)\n",
        "\n",
        "\n",
        "def train(epoch,train_loader, model, optimizer, lam):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        output = model(data)\n",
        "\n",
        "        l1_norm = sum(i.abs().sum()for i in model.parameters())\n",
        "        loss = CrossELoss(output, one_hot(target))\n",
        "        loss += (lam * l1_norm)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def validation(validation_loader, model):\n",
        "  model.eval()\n",
        "  validation_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in validation_loader:\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "      output = model(data)\n",
        "      pred = output.data.max(1)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "      validation_loss += CrossELoss(output, one_hot(target)).item()\n",
        "      validation_loss /= len(validation_loader.dataset)\n",
        "\n",
        "def test(test_loader,model):\n",
        "  loss = nn.CrossEntropyLoss()\n",
        "  model.eval()\n",
        "  test_loss,correct = 0, 0\n",
        "  p_labels, gt_labels = [], []\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "      output = model(data)\n",
        "      test_loss += CrossELoss(output, one_hot(target)).item()\n",
        "      pred = output.data.max(1)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "      p_labels.append(pred)\n",
        "      gt_labels.append(one_hot(target).argmax(1))\n",
        "\n",
        "  p_labels = torch.stack(p_labels, dim = 0)\n",
        "  gt_labels = torch.stack(gt_labels, dim = 0)\n",
        "  return p_labels, gt_labels\n",
        "\n",
        "def logistic_regression(dataset_name):\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    n_epochs = 10\n",
        "\n",
        "\n",
        "    p_labels, gt_labels = [], []\n",
        "\n",
        "\n",
        "    if dataset_name == 'MNIST':\n",
        "      MNIST_training = torchvision.datasets.MNIST(root = './data', train=True, download=True,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "      MNIST_test_set = torchvision.datasets.MNIST(root = './data', train=False, download=True,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "\n",
        "      MNIST_training_set, MNIST_validation_set = torch.utils.data.random_split(MNIST_training, [48000, 12000], generator = torch.Generator().manual_seed(42))\n",
        "\n",
        "\n",
        "      train_loader = torch.utils.data.DataLoader(MNIST_training_set,batch_size=75, shuffle=True)\n",
        "\n",
        "      validation_loader = torch.utils.data.DataLoader(MNIST_validation_set,batch_size=75, shuffle=True)\n",
        "\n",
        "      test_loader = torch.utils.data.DataLoader(MNIST_test_set,batch_size=1000, shuffle=True)\n",
        "\n",
        "      model = LogisticRegression(28*28,10).to(device)\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "      validation(validation_loader,model)\n",
        "      for epoch in (1, n_epochs+1):\n",
        "\n",
        "        train(epoch, train_loader, model,optimizer,l1_lambda_1)\n",
        "        validation(validation_loader,model)\n",
        "      p_labels,  gt_labels  = test(test_loader, model)\n",
        "\n",
        "\n",
        "    elif dataset_name == \"CIFAR10\":\n",
        "      CIFAR10_training = torchvision.datasets.CIFAR10(root = './data', train=True, download=True,\n",
        "                              transform=torchvision.transforms.Compose([\n",
        "                                 torchvision.transforms.ToTensor(),\n",
        "                                 torchvision.transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))]))\n",
        "\n",
        "      CIFAR10_test_set = torchvision.datasets.CIFAR10(root = './data', train=False, download=True,\n",
        "                              transform=torchvision.transforms.Compose([\n",
        "                                 torchvision.transforms.ToTensor(),\n",
        "                                 torchvision.transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))]))\n",
        "\n",
        "      CIFAR10_training_set, CIFAR10_validation_set = random_split(CIFAR10_training, [38000, 12000],generator = torch.Generator().manual_seed(42))\n",
        "\n",
        "\n",
        "      train_loader = torch.utils.data.DataLoader(CIFAR10_training_set,batch_size=70, shuffle=True)\n",
        "\n",
        "      validation_loader = torch.utils.data.DataLoader(CIFAR10_validation_set,batch_size=70, shuffle=True)\n",
        "\n",
        "      test_loader = torch.utils.data.DataLoader(CIFAR10_test_set,batch_size=1000, shuffle=True)\n",
        "\n",
        "      model = LogisticRegression(32*32*3,10).to(device)\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "      validation(validation_loader,model)\n",
        "      for epoch in (1, n_epochs+1):\n",
        "\n",
        "        train(epoch, train_loader, model,optimizer,l1_lambda_1)\n",
        "        validation(validation_loader,model)\n",
        "      p_labels,  gt_labels  = test(test_loader, model)\n",
        "\n",
        "\n",
        "    return p_labels.cpu(), gt_labels.cpu()\n",
        "\n",
        "def tune_hyper_parameter():\n",
        "  n_epochs = 7\n",
        "  CIFAR10_training = torchvision.datasets.CIFAR10(root = './data', train=True, download=True,\n",
        "                                    transform=torchvision.transforms.Compose([\n",
        "                                      torchvision.transforms.ToTensor(),\n",
        "                                      torchvision.transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))]))\n",
        "\n",
        "  CIFAR10_test_set = torchvision.datasets.CIFAR10(root = './data', train=False, download=True,\n",
        "                                  transform=torchvision.transforms.Compose([\n",
        "                                    torchvision.transforms.ToTensor(),\n",
        "                                    torchvision.transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))]))\n",
        "\n",
        "\n",
        "  CIFAR10_training_set, CIFAR10_validation_set = random_split(CIFAR10_training, [38000, 12000])\n",
        "\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(CIFAR10_training_set,batch_size=128, shuffle=True,num_workers=2)\n",
        "\n",
        "  validation_loader = torch.utils.data.DataLoader(CIFAR10_validation_set,batch_size=128, shuffle=True,num_workers=2)\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(CIFAR10_test_set,batch_size=1000, shuffle=True,num_workers=2)\n",
        "\n",
        "  model = LogisticRegression(32*32*3,10).to(device)\n",
        "\n",
        "  SGD_optimizer_1 = torch.optim.SGD(model.parameters(), lr =0.001)\n",
        "  SGD_optimizer_2 = torch.optim.SGD(model.parameters(), lr =0.00001)\n",
        "\n",
        "  Adam_optimizer_1 = torch.optim.Adam(model.parameters(), lr =0.001)\n",
        "  Adam_optimizer_2 = torch.optim.Adam(model.parameters(), lr =0.00001)\n",
        "\n",
        "  optimizers = [SGD_optimizer_1,SGD_optimizer_2,Adam_optimizer_1,\n",
        "                Adam_optimizer_2]\n",
        "\n",
        "  run_time = 0\n",
        "\n",
        "  l1_lambda = [0.00045,0.00003]\n",
        "  best_acc = 0\n",
        "  best_model = None\n",
        "  tuned_lambda = 0\n",
        "  correct = 0\n",
        "  start = timeit.default_timer()\n",
        "  for i in optimizers:\n",
        "      for j in l1_lambda:\n",
        "\n",
        "        model = LogisticRegression(32*32*3,10).to(device)\n",
        "\n",
        "        optimizer = i\n",
        "        lambda_1 = j\n",
        "\n",
        "        for epoch in range(1, n_epochs+1):\n",
        "            train(epoch,train_loader,model,optimizer,lambda_1 )\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in validation_loader:\n",
        "                data = data.to(device)\n",
        "                target = target.to(device)\n",
        "                output = model(data)\n",
        "                pred = output.data.max(1)[1]\n",
        "                correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "            accuracy = (correct * 100.0 )/(len(validation_loader.dataset))\n",
        "\n",
        "            if accuracy > best_acc:\n",
        "                  best_acc = accuracy\n",
        "                  tuned_optimizer = i\n",
        "                  tuned_lambda = j\n",
        "                  best_model =  model\n",
        "  stop = timeit.default_timer()\n",
        "  run_time = stop - start\n",
        "\n",
        "  tuned_parameters = [tuned_lambda,tuned_optimizer]\n",
        "  print(\"-------------------\")\n",
        "  print(\"CIFAR10 tuned\")\n",
        "\n",
        "  return run_time,tuned_parameters,best_acc\n",
        "\n",
        "\n",
        "\"\"\"Main loop. Run time and total score will be shown below.\"\"\"\n",
        "\n",
        "def run_on_dataset(dataset_name, filename):\n",
        "    if dataset_name == \"MNIST\":\n",
        "        min_thres = 0.82\n",
        "        max_thres = 0.92\n",
        "\n",
        "    elif dataset_name == \"CIFAR10\":\n",
        "        min_thres = 0.28\n",
        "        max_thres = 0.38\n",
        "\n",
        "    correct_predict, accuracy, run_time = run(logistic_regression, dataset_name, filename)\n",
        "\n",
        "    score = compute_score(accuracy, min_thres, max_thres)\n",
        "    result = OrderedDict(correct_predict=correct_predict,\n",
        "                         accuracy=accuracy, score=score,\n",
        "                         run_time=run_time)\n",
        "    return result, score\n",
        "\n",
        "\n",
        "def main():\n",
        "    filenames = { \"MNIST\": \"predictions_mnist.txt\", \"CIFAR10\": \"predictions_cifar10.txt\"}\n",
        "    result_all = OrderedDict()\n",
        "    score_weights = [0.5, 0.5]\n",
        "    scores = []\n",
        "    for dataset_name in [\"MNIST\",\"CIFAR10\"]:\n",
        "        result_all[dataset_name], this_score = run_on_dataset(dataset_name, filenames[dataset_name])\n",
        "        scores.append(this_score)\n",
        "    total_score = [score * weight for score, weight in zip(scores, score_weights)]\n",
        "    total_score = np.asarray(total_score).sum().item()\n",
        "    result_all['total_score'] = total_score\n",
        "\n",
        "\n",
        "main()\n",
        "tune_hyper_parameter()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvs5hUgx380N",
        "outputId": "9a5d2231-a4c4-4799-e99c-96bb68a296c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 91 %\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Accuracy of the network on the 10000 test images: 38 %\n",
            "\n",
            "Result:\n",
            " OrderedDict([   (   'MNIST',\n",
            "                    OrderedDict([   ('correct_predict', 9141),\n",
            "                                    ('accuracy', 0.9141),\n",
            "                                    ('score', 94.09999999999998),\n",
            "                                    ('run_time', 16.66946894800003)])),\n",
            "                (   'CIFAR10',\n",
            "                    OrderedDict([   ('correct_predict', 3843),\n",
            "                                    ('accuracy', 0.3843),\n",
            "                                    ('score', 100.0),\n",
            "                                    ('run_time', 33.64073344899998)])),\n",
            "                ('total_score', 97.04999999999998)])\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    }
  ]
}